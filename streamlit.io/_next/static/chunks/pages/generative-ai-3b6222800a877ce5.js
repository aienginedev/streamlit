(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[47967],{84315:function(e,t,n){let a=n(67294);e.exports={attributes:{pageTitle:"Build powerful apps using generative AI & LLMs",pageDescription:"Your go-to platform to create, deploy, and share LLM-powered apps quickly.",heroTitleHtml:"Build powerful generative AI apps",heroTextHtml:"Thousands of developers use Streamlit as their go-to platform to experiment and build generative AI apps. Create, deploy, and share LLM-powered apps as fast as ChatGPT can compute!",primaryCtaTextHtml:"Try example code",secondaryCtaTextHtml:"Deploy on Community Cloud",primaryCtaLink:"https://llm-examples.streamlit.app/",secondaryCtaLink:"/cloud",featuredApps:{title:"Your LLM code playground",apps:[{demoApp:null,title:"Build a chatbot",code:'```python\n# First\nimport openai import streamlit as st\nwith st.sidebar:\n    openai_api_key = st.text_input("OpenAI API Key", key="chatbot_api_key", type="password")\n    "[Get an OpenAI API key](https://platform.openai.com/account/api-keys)"\n    "[View the source code](https://github.com/streamlit/llm-examples/blob/main/Chatbot.py)"\n    "[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/streamlit/llm-examples?quickstart=1)"\n\nst.title("\uD83D\uDCAC Chatbot") if "messages" not in st.session_state:\n    st.session_state["messages"] = [{"role": "assistant", "content": "How can I help you?"}]\n\nfor msg in st.session_state.messages:\n    st.chat_message(msg["role"]).write(msg["content"])\n\nif prompt := st.chat_input():\n    if not openai_api_key:\n        st.info("Please add your OpenAI API key to continue.")\n        st.stop()\n\n    openai.api_key = openai_api_key\n    st.session_state.messages.append({"role": "user", "content": prompt})\n    st.chat_message("user").write(prompt)\n    response = openai.ChatCompletion.create(model="gpt-3.5-turbo", messages=st.session_state.messages)\n    msg = response.choices[0].message\n    st.session_state.messages.append(msg)\n    st.chat_message("assistant").write(msg.content)\n\n```',iframe:'<iframe width="560" height="315" src="https://llm-examples.streamlit.app/?embed=True" title="Chatbot Example" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>'},{title:"File Q&A with Anthropic",code:'```python\n# Second\nimport streamlit as st import anthropic\nwith st.sidebar:\n    anthropic_api_key = st.text_input("Anthropic API Key", key="file_qa_api_key", type="password")\n    "[View the source code](https://github.com/streamlit/llm-examples/blob/main/pages/1_File_Q%26A.py)"\n    "[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/streamlit/llm-examples?quickstart=1)"\n\nst.title("\uD83D\uDCDD File Q&A with Anthropic") uploaded_file = st.file_uploader("Upload an article", type=("txt", "md")) question = st.text_input(\n    "Ask something about the article",\n    placeholder="Can you give me a short summary?",\n    disabled=not uploaded_file,\n)\nif uploaded_file and question and not anthropic_api_key:\n    st.info("Please add your Anthropic API key to continue.")\n\nif uploaded_file and question and anthropic_api_key:\n    article = uploaded_file.read().decode()\n    prompt = f"""{anthropic.HUMAN_PROMPT} Here\'s an article:\\n\\n<article>\n    {article}\\n\\n</article>\\n\\n{question}{anthropic.AI_PROMPT}"""\n\n    client = anthropic.Client(api_key=anthropic_api_key)\n    response = client.completions.create(\n        prompt=prompt,\n        stop_sequences=[anthropic.HUMAN_PROMPT],\n        model="claude-v1", #"claude-2" for Claude 2 model\n        max_tokens_to_sample=100,\n    )\n    st.write("### Answer")\n    st.write(response.completion)\n\n```',iframe:'<iframe src="https://llm-examples.streamlit.app/File_Q&A?embed=True" height="450" style="width:100%;border:none;"></iframe>'},{title:"Search with Langchain",code:'```python\n\n  import streamlit as st\n\n  from langchain.agents import initialize_agent, AgentType\n  from langchain.callbacks import StreamlitCallbackHandler\n  from langchain.chat_models import ChatOpenAI\n  from langchain.tools import DuckDuckGoSearchRun\n\n  with st.sidebar:\n      openai_api_key = st.text_input("OpenAI API Key", key="langchain_search_api_key_openai", type="password")\n      "[Get an OpenAI API key](https://platform.openai.com/account/api-keys)"\n      "[View the source code](https://github.com/streamlit/llm-examples/blob/main/pages/2_Chat_with_search.py)"\n      "[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/streamlit/llm-examples?quickstart=1)"\n\n  st.title("\uD83D\uDD0E LangChain - Chat with search")\n\n  """\n  In this example, we\'re using `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an interactive Streamlit app.\n  Try more LangChain \uD83E\uDD1D Streamlit Agent examples at [github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n  """\n\n  if "messages" not in st.session_state:\n      st.session_state["messages"] = [\n          {"role": "assistant", "content": "Hi, I\'m a chatbot who can search the web. How can I help you?"}\n      ]\n\n  for msg in st.session_state.messages:\n      st.chat_message(msg["role"]).write(msg["content"])\n\n  if prompt := st.chat_input(placeholder="Who won the Women\'s U.S. Open in 2018?"):\n      st.session_state.messages.append({"role": "user", "content": prompt})\n      st.chat_message("user").write(prompt)\n\n      if not openai_api_key:\n          st.info("Please add your OpenAI API key to continue.")\n          st.stop()\n\n      llm = ChatOpenAI(model_name="gpt-3.5-turbo", openai_api_key=openai_api_key, streaming=True)\n      search = DuckDuckGoSearchRun(name="Search")\n      search_agent = initialize_agent([search], llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, handle_parsing_errors=True)\n      with st.chat_message("assistant"):\n          st_cb = StreamlitCallbackHandler(st.container(), expand_new_thoughts=False)\n          response = search_agent.run(st.session_state.messages, callbacks=[st_cb])\n          st.session_state.messages.append({"role": "assistant", "content": response})\n          st.write(response)\n\n```',iframe:'<iframe src="https://llm-examples.streamlit.app/Chat_with_search?embed=True" height="450" style="width:100%;border:none;"></iframe>'},{title:"Langchain Quickstart App",code:'```python\n\n  import streamlit as st\n  from langchain.llms import OpenAI\n\n  st.title("\uD83E\uDD9C\uD83D\uDD17 Langchain Quickstart App")\n\n  with st.sidebar:\n      openai_api_key = st.text_input("OpenAI API Key", type="password")\n      "[Get an OpenAI API key](https://platform.openai.com/account/api-keys)"\n\n\n  def generate_response(input_text):\n      llm = OpenAI(temperature=0.7, openai_api_key=openai_api_key)\n      st.info(llm(input_text))\n\n\n  with st.form("my_form"):\n      text = st.text_area("Enter text:", "What are 3 key advice for learning how to code?")\n      submitted = st.form_submit_button("Submit")\n      if not openai_api_key:\n          st.info("Please add your OpenAI API key to continue.")\n      elif submitted:\n          generate_response(text)\n```',iframe:'<iframe src="https://llm-examples.streamlit.app/Langchain_Quickstart?embed=True" height="450" style="width:100%;border:none;"></iframe>'},{title:"Langchain - Blog Outline Generator App",code:'```python\n\nimport streamlit as st\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\n\nst.title("\uD83E\uDD9C\uD83D\uDD17 Langchain - Blog Outline Generator App")\n\nopenai_api_key = st.sidebar.text_input("OpenAI API Key", type="password")\n\n\ndef blog_outline(topic):\n    # Instantiate LLM model\n    llm = OpenAI(model_name="text-davinci-003", openai_api_key=openai_api_key)\n    # Prompt\n    template = "As an experienced data scientist and technical writer, generate an outline for a blog about {topic}."\n    prompt = PromptTemplate(input_variables=["topic"], template=template)\n    prompt_query = prompt.format(topic=topic)\n    # Run LLM model\n    response = llm(prompt_query)\n    # Print results\n    return st.info(response)\n\n\nwith st.form("myform"):\n    topic_text = st.text_input("Enter prompt:", "")\n    submitted = st.form_submit_button("Submit")\n    if not openai_api_key:\n        st.info("Please add your OpenAI API key to continue.")\n    elif submitted:\n        blog_outline(topic_text)\n\n\n```',iframe:'<iframe src="https://llm-examples.streamlit.app/Langchain_PromptTemplate?embed=True" height="450" style="width:100%;border:none;"></iframe>'}],linkText:"",linkUrl:"",copy:"Play around with a few of Streamlit’s open source examples using LLMs."},appsHeadline:"Unlock your creativity with LLMs",cardsGrid:{title:"Works with everything in the AI ecosystem",subtitle:"",followUp:"",cards:[{title:"OpenAI",logo:"images/uploads/openai.png",url:"https://github.com/openai/openai-cookbook/blob/main/apps/embeddings-playground/README.md",highlighted:!1,itemWidth:"2"},{title:"Stable Diffusion",logo:"images/uploads/stable.png",url:"https://github.com/Stability-AI/stablediffusion/tree/main/scripts/streamlit",highlighted:!1,itemWidth:"2"},{title:"LangChain",logo:"images/uploads/lang.png",url:"https://github.com/langchain-ai/streamlit-agent",highlighted:!0,itemWidth:"2"},{title:"BabyAGI",logo:"images/uploads/baby.png",url:"https://github.com/dory111111/babyagi-streamlit",highlighted:!1,itemWidth:"2"},{title:"llamaindex",logo:"images/uploads/llama.png",url:"https://gpt-index.readthedocs.io/en/latest/end_to_end_tutorials/question_and_answer/terms_definitions_tutorial.html",highlighted:!1,itemWidth:"2"},{title:"Sygil-Dev",logo:"images/uploads/dev.png",url:"https://github.com/Sygil-Dev/sygil-webui",highlighted:!1,itemWidth:"2"},{title:"Hugging Face",logo:"images/uploads/face.png",url:"https://huggingface.co/docs/hub/spaces-sdks-streamlit",highlighted:!1,itemWidth:"2"},{title:"Pinecone",logo:"images/uploads/pinecone.png",url:"https://blog.streamlit.io/finding-your-look-alikes-with-semantic-search/",highlighted:!1,itemWidth:"2"},{title:"Snowflake",logo:"images/uploads/snowflake-logo.png",url:"https://developers.snowflake.com/demos/data-exploration-llm-chatbot/",highlighted:!1,itemWidth:"2"},{title:"Trubrics",logo:"images/uploads/trubrics-logo.png",url:"https://trubrics.github.io/trubrics-sdk/",highlighted:!1,itemWidth:"2"}]},videoCta:{eyebrow:"Did you know...",headline:"LLMs are genius at writing apps",copy:" Streamlit is a breeze for humans and AI alike. Just watch ChatGPT write a Streamlit app in a matter of seconds. It's pair programming like never before.",video:"/videos/llm.mp4",thumbnail:"images/uploads/llms-ai.png"},blog:{title:"Get tips & tutorials from the community",linkText:"Read All →",linkUrl:"https://blog.streamlit.io/tag/llms/",blogTags:["LLMs"],blogPosts:12},cardsCta:{title:"How you can get involved",cards:[{emoji:"\uD83C\uDFA4",title:"Become an Advocate",text:"Inspire Python developers and data scientists around the world with your expertise. We’ll help you become famous!",link:{url:"/community/advocates",text:"Learn More"}},{emoji:"\uD83D\uDCBB",title:"Become a Creator",text:"A select group with exclusive access to Streamlit developers, betas, and other exclusive perks that distinguishes them in the community.",link:{url:"/creators",text:"Learn More"}},{emoji:"\uD83C\uDF93",title:"Become a Student Ambassador",text:"Join an active group of students enhancing their leadership skills, hosting hackathons, and gaining social media influence.",link:{url:"/community/ambassadors",text:"Learn More"}}]},faqBlock:{headline:"FAQ Section",faqs:[{title:"What is a Large Language Model (LLM)?",content:"A large language model is a **kind of neural network** – so-called Transformer – with a **large number of parameters** and **trained on a huge corpus of natural language data** which enable general-purpose, programmable AI. LLMs cost **10-100s of millions of dollars** to train and include **100s of billions of parameters**, making them the most complex AI projects built by humanity.\n\n![](images/uploads/llm-model.png)"},{title:"Which LLMs work with Streamlit?",content:"Any LLM with a python implementation or API call works with Streamlit. This includes popular open source and closed source models like [OpenAI](/9e4c23c9964e49908f330fccf15b8da9?pvs=25), LLaMa, Anthropic, PaLM, and other open source models such as the ones hosted on HuggingFace or built with Transformers"},{title:"How to obtain an OpenAI API key?",content:"You can get your own OpenAI API key by following the following instructions\n\n* Go to <https://platform.openai.com/account/api-keys>.\n* Click on the + Create new secret key button.\n* Next, enter an identifier name (optional) and click on the Create secret key button."},{title:"How to load LLM API key in a Streamlit app",content:'Then add your OpenAI API key as an environment variable in Streamlit Community Cloud:\n\n* At the lower right corner, click on < Manage app then click on the vertical "..." followed by clicking on Settings.\n* This brings the **App settings**, next, click on the Secrets tab and paste the API key into the text box.\n* Click Save\n\nNow, that we have the API key defined in secrets, it should now be automatically detected in-app from the `OPENAI_API_KEY` environment variable. This can be explicitly called via `st.secrets[“OPENAI_API_KEY”]`'},{title:"How do I connect Streamlit to OpenAI?",content:'You can use the OpenAI python library.\n\nRun `pip install openai streamlit`. Here’s a very simple example:\n\n```python\nimport streamlit as st\n\nimport openai\n\nopenai.api_key = st.secrets.openai_api_key\n\ninput = st.text_input("Ask a question")\n\nif st.button("Submit"):\n\nchat_completion = openai.ChatCompletion.create(model="gpt-3.5-turbo", messages=[{"role": "user", "content": input}])\n\nst.write(chat_completion.choices[0].message.content)\n```'},{title:"How do I connect Streamlit to an LLM?",content:'You can connect any other LLM to Streamlit the same way you would connect to the LLM from any other Python application. Search for a python client or example code from the LLM, and adapt it to Streamlit similar to the example above.\n\nAnother good option is to use LangChain, which maintains [integrations to a huge range of LLMs](https://python.langchain.com/en/latest/modules/models/llms/integrations.html) for Python.\n\n* For other data sets, st.connection makes it easier to connect your LLM app to tools and data (already out for Snowflake, SQL connections; vector store & LangChain integrations coming in 2 months post Summit). It’s a 2 line "import + initialize".'}]},footerCta:{eyebrow:"Share your LLM apps with the community!",title:"Become a Streamlit Advocate",subhead:"With an unwavering commitment to open source, join us in our mission to make Streamlit the go-to platform for building LLM apps.",link:"https://discuss.streamlit.io",link_text:"Join the forum"}},react:function(e){Object.keys(e).forEach(function(t){this[t]=e[t]});let t=a.createElement("div",{className:"frontmatter-markdown"});return t}}},89246:function(e,t,n){(window.__NEXT_P=window.__NEXT_P||[]).push(["/generative-ai",function(){return n(42800)}])},56817:function(e,t,n){"use strict";var a=n(85893),i=n(94184),s=n.n(i);let o=e=>{let{text:t="Get Started",href:n="/",target:i="_self",color:o="bg-green-50",hoverColor:l="hover:bg-green-40",activeColor:r="active:bg-green-40",classes:c}=e;return(0,a.jsx)("div",{children:(0,a.jsx)("a",{href:n,target:i,className:s()("".concat(l||""),"".concat(r||""),"".concat(o||""),"".concat(c||"")),children:t})})};t.Z=o},88798:function(e,t,n){"use strict";var a=n(85893),i=n(56817);let s=e=>{let{cta:t}=e;return(0,a.jsx)("section",{className:"bg-gray-90",children:(0,a.jsx)("div",{className:"sm:grid sm:grid-cols-12 gap-x-8 container mx-auto gap-8 lg:gap-y-16 py-16 md:py-28 px-6 sm:px-8",children:(0,a.jsxs)("div",{className:"flex flex-col gap-8 col-start-1 col-end-13 lg:col-start-2 lg:col-end-12 xl:col-start-3 xl:col-end-11 md:text-center",children:[(0,a.jsxs)("div",{className:"flex flex-col gap-4",children:[t.eyebrow&&(0,a.jsx)("div",{className:"uppercase tracking-wider font-bold sm:text-lg text-green-50",children:t.eyebrow}),(0,a.jsx)("h2",{className:"text-4xl sm:text-7xl whitespace-pre-line text-white font-bold",children:t.title})]}),(0,a.jsx)("div",{className:"text-gray-60 text-2xl",children:(0,a.jsx)("p",{children:t.subhead})}),(0,a.jsx)("div",{className:"text-left md:text-center",children:(0,a.jsx)(i.Z,{target:"_blank",text:t.link_text,href:t.link,hoverColor:"hover:bg-green-40",activeColor:"active:bg-green-40",color:"bg-green-50",classes:"px-10 py-2.5 rounded-md text-2xl text-gray-90 inline-block transition-colors"})})]})})})};t.Z=s},42800:function(e,t,n){"use strict";n.r(t),n.d(t,{__N_SSG:function(){return Z},default:function(){return D}});var a=n(85893),i=n(98650),s=n(80886),o=n(64563),l=n(1863),r=n(94184),c=n.n(r),p=n(67294),m=n(84737),d=n(5152),h=n.n(d),u=n(28051),g=n(27984),x=n.n(g);let f=h()(()=>Promise.all([n.e(21823),n.e(1940)]).then(n.bind(n,89208)),{loadableGenerated:{webpack:()=>[89208]},ssr:!1});function b(e){let{data:t}=e,[n,i]=(0,p.useState)(0);return(0,a.jsxs)(m.$0,{topPadding:"medium",bottomPadding:"small",width:"full",layout:"none",centered:!1,children:[(0,a.jsxs)("div",{className:c()(m.uQ,m.Gz),children:[(0,a.jsx)(m.NZ,{className:"text-gray-90 max-w-3xl mt-8 md:mt-0 md:mb-8",children:t.title}),t.copy&&(0,a.jsx)("div",{className:"text-gray-70 text-2xl",dangerouslySetInnerHTML:{__html:t.copy}}),t.linkText&&(0,a.jsx)("a",{href:t.linkUrl,rel:"noopener noreferrer",className:c()(m.NY,"".concat(t.color||"text-indigo-60"),"hover:".concat(t.hoverColor||"text-indigo-50"),"font-semibold"),children:t.linkText})]}),(0,a.jsx)(m.JQ,{classes:"pb-0 mb-6",children:(0,a.jsxs)("ul",{className:"flex mt-8 mb-6",children:[t.apps&&t.apps.map((e,t)=>(0,a.jsx)("li",{className:"flex-none mr-8",onClick:()=>i(t),children:(0,a.jsx)("button",{className:c()("border-2 px-8 py-4 rounded-md font-semibold hover:border-indigo-70",n===t?"border-indigo-70 text-gray-100":"border-gray-40 text-gray-70"),children:(0,a.jsx)("p",{children:e.title})})},t)),t.apps&&t.apps.length>3&&(0,a.jsx)("li",{className:"flex-none w-8"})]})}),(0,a.jsxs)("div",{className:c()(m.uQ,m.Gz,"flex flex-col md:flex-row"),children:[(0,a.jsx)("div",{className:"w-full mb-8 md:mb-0 md:w-7/12 overflow-hidden",children:(0,a.jsx)(u.D,{components:{code(e){var t;let{node:n,...i}=e,s=i.inline,o=i.children[0];if(s)return(0,a.jsx)("code",{children:o});let l=(null==i?void 0:null===(t=i.className)||void 0===t?void 0:t.length)?i.className.substring(9):"python";return(0,a.jsx)(f,{code:o,language:l,lines:!0,typeOn:!0})}},className:c()(x().Code),children:t.apps[n].code})}),(0,a.jsx)("div",{className:c()("w-full md:w-5/12 md:ml-8",x().iframeContainer),dangerouslySetInnerHTML:{__html:t.apps[n].iframe}})]})]})}var y=n(21320),_=n(41664),w=n.n(_),k=n(97623),v=n.n(k);function A(e){let{title:t,subtitle:n,cards:i,followUp:s}=e,o=e=>{let t="col-span-2";switch(parseInt(e)){case 1:return"col-span-1";case 2:return t+" lg:col-span-2";case 3:return t+" lg:col-span-3";case 4:return t+" lg:col-span-4";case 5:return t+" lg:col-span-5";case 6:return t+" lg:col-span-6"}};return(0,a.jsxs)(m.$0,{layout:"column",topPadding:"medium",bottomPadding:"large",className:v().rootEl,children:[(0,a.jsx)(m.NZ,{className:"text-gray-90 max-w-3xl mt-8 md:mt-0 md:mb-8",children:t}),n&&(0,a.jsx)("p",{className:c()(m.NY,"font-marker","text-indigo-60 sm:-mb-6"),children:n}),(0,a.jsx)("ul",{className:"grid grid-flow-row-dense grid-cols-2 md:grid-cols-4 lg:grid-cols-6 gap-3 md:gap-8",children:i.map((e,t)=>(0,a.jsx)("li",{className:c()("".concat(o(e.itemWidth||1)),"bg-white","rounded","shadow-md md:shadow-xl","flex justify-stretch items-stretch","hover:shadow transition duration-75"),children:(0,a.jsx)(w(),{href:e.url,className:"flex justify-center items-center flex-auto p-3 md:p-8",target:"_blank",rel:"noopener noreferrer",children:(0,a.jsx)("img",{src:"".concat(e.logo,"?nf_resize=fit&h=56"),srcSet:"".concat(e.logo,"?nf_resize=fit&h=56 1x, ").concat(e.logo,"?nf_resize=fit&h=112 2x"),alt:e.title,className:"h-16 mx-auto object-contain w-full",width:"300",height:"35",loading:"lazy"})})},t))}),(0,a.jsx)(m.Wr,{className:"whitespace-pre-line",dangerouslySetInnerHTML:{__html:s}})]})}var j=n(83824),L=n(18610),I=n.n(L);function N(e){let{eyebrow:t,headline:n,copy:i,video:s,thumbnail:o}=e,[l,r]=(0,p.useState)(!0),d=(0,p.useRef)(null),h=()=>{d.current.play().then(()=>r(!1))},u=()=>{d.current.pause()};return(0,a.jsxs)(m.$0,{layout:"column",topPadding:"tiny",bottomPadding:"tiny",centered:!0,className:c()(I().rootEl,"bg-indigo-10 xl:rounded-3xl"),children:[(0,a.jsxs)("div",{className:"flex flex-col center gap-4",children:[(0,a.jsx)("div",{className:"uppercase tracking-wider font-marker sm:text-lg text-indigo-70",children:t}),(0,a.jsx)(m.NZ,{className:"text-gray-90",children:n}),i&&(0,a.jsx)(m.Wr,{dangerouslySetInnerHTML:{__html:i}})]}),(0,a.jsx)("div",{className:c()("relative","col-span-full","rounded-2xl","overflow-hidden","border border-gray-80 border-opacity-20","shadow-2xl"),children:(0,a.jsx)(j.h,{onEnter:h,onLeave:u,bottomOffset:"30%",children:(0,a.jsx)("video",{muted:!0,controls:!0,loop:!0,className:"w-full",ref:d,children:(0,a.jsx)("source",{src:s})})})})]})}var C=n(67414),P=n(53442);function S(e){let{title:t,linkText:n,linkUrl:i,blogPosts:s,blogTags:o}=e,l=new C.Z({url:"https://streamlit.ghost.io",key:"b48c60c1a281bf21dc9afa9950",version:"v5.0"}),[r,d]=(0,p.useState)([]),[h,u]=(0,p.useState)(!0);return(0,p.useEffect)(()=>{let e=[];o.forEach(t=>{let n=t.trim();e.push(n)}),l.posts.browse({limit:s||6,filter:"tag:[".concat(e.join(","),"]")}).then(e=>{let t=e.map(e=>({linkURL:e.url,image:e.feature_image,text:e.title}));d(t),u(!1)}).catch(e=>{console.error(e)})},[]),(0,a.jsx)(a.Fragment,{children:h&&(0,a.jsx)("div",{className:"flex justify-center items-center h-96",children:(0,a.jsx)("div",{className:"animate-spin rounded-full h-32 w-32 border-t-2 border-b-2 border-indigo-90"})})||(0,a.jsx)(P.Z,{title:t,linkText:n,linkUrl:i,color:"text-indigo-60",hoverColor:"text-indigo-50",boxes:r.map((e,t)=>(0,a.jsxs)("a",{href:e.linkURL,target:"_blank",rel:"noopener noreferrer block",className:"group",children:[(0,a.jsx)("div",{className:c()("rounded-xl","aspect[2/1]","shadow-xl hover:shadow","transition-shadow","flex"),children:(0,a.jsx)("img",{src:e.image,alt:"screenshot",className:"w-full h-full rounded-lg object-cover"})}),(0,a.jsx)("p",{className:c()(m.NY,"tracking-tight sm:tracking-tight","font-semibold text-gray-90 mt-6"),children:e.text})]},t))})})}n(23570);var T=n(73666),O=n(86641),M=n.n(O);let H=h()(()=>Promise.all([n.e(21823),n.e(1940)]).then(n.bind(n,89208)),{loadableGenerated:{webpack:()=>[89208]},ssr:!1});function E(e){let{headline:t,faqs:n}=e;return(0,a.jsxs)(m.$0,{layout:"column",width:"narrow",topPadding:"none",children:[(0,a.jsx)("div",{className:"text-center",children:(0,a.jsx)(m.NZ,{className:"text-gray-90 sm:leading-none",children:t})}),(0,a.jsx)("ul",{children:n.map((e,t)=>(0,a.jsx)(q,{faq:e},t))})]})}let G={code(e){var t;let{node:n,...i}=e,s=i.inline,o=i.children[0];if(s)return(0,a.jsx)("code",{children:o});let l=(null==i?void 0:null===(t=i.className)||void 0===t?void 0:t.length)?i.className.substring(9):"python";return(0,a.jsx)(H,{code:o,language:l})}};function q(e){let{faq:t}=e,[n,i]=(0,p.useState)(!1),s=(0,p.useCallback)(e=>{i(!n)},[n,i]);return(0,a.jsxs)("li",{className:"border border-gray-40 rounded-md p-4 pr-6 mb-4",children:[(0,a.jsxs)("div",{className:"flex items-center toggle group",onClick:s,children:[(0,a.jsx)("h6",{className:"cursor-pointer flex-1 font-medium text-gray-90 text-lg leading-tight sm:text-xl group-hover:text-indigo-70 pr-4 toggle",children:t.title}),(0,a.jsx)(T.r,{className:c()("transform transition-transform toggle cursor-pointer",{"rotate-180":n})})]}),(0,a.jsx)("div",{className:c()(M().Faq,"mt-2",{hidden:!n}),children:(0,a.jsx)(u.D,{components:G,children:t.content})})]})}var W=n(88798),F=n(84315);let R=e=>{let{allAppsData:t}=e,{pageTitle:n,pageDescription:r,sharingImage:c,heroTitleHtml:p,heroTextHtml:d,primaryCtaTextHtml:h,secondaryCtaTextHtml:u,primaryCtaLink:g,secondaryCtaLink:x,appsHeadline:f,featuredApps:_,videoCta:w,blog:k,faqBlock:v,cardsGrid:j,footerCta:L}=F.attributes;return(0,a.jsxs)(i.Z,{children:[(0,a.jsx)(s.Z,{pageTitle:n,pageDescription:r,sharingImage:c}),(0,a.jsx)(o.Z,{title:p,text:d,bottomPadding:"none",children:(0,a.jsx)(l.Z,{primaryCtaText:h,secondaryCtaText:u,primaryCtaLink:g,secondaryCtaLink:x,accentColor:"bg-indigo-70"})}),(0,a.jsx)(b,{data:_}),(0,a.jsxs)(m.$0,{topPadding:"medium",bottomPadding:"small",children:[(0,a.jsx)(m.NZ,{className:"text-gray-90 md:mb-8",children:f}),(0,a.jsx)(y.Z,{activePage:"using-llms",apps:t,categories:["llms"],defaultCategory:"llms",appsPerPage:12,appsPerRow:4,activeCategory:"llms",setActiveCategory(){},hideSideBar:!0,colors:{text:{base:"text-gray-70",hover:"hover:text-gray-90"},active:{bg:"bg-gray-20",text:"text-black-70",hover:"hover:text-gray-90"}}})]}),(0,a.jsx)(A,{title:j.title,subtitle:j.subtitle,cards:j.cards,followUp:j.followUp}),(0,a.jsx)(N,{eyebrow:w.eyebrow,headline:w.headline,copy:w.copy,video:w.video,thumbnail:w.thumbnail}),(0,a.jsx)(S,{title:k.title,linkText:k.linkText,linkUrl:k.linkUrl,blogPosts:k.blogPosts,blogTags:k.blogTags}),(0,a.jsx)(E,{headline:v.headline,faqs:v.faqs}),L&&(0,a.jsx)(W.Z,{cta:L})]})};var Z=!0,D=R},97623:function(e){e.exports={rootEl:"CardGrid_rootEl__cf3tf"}},86641:function(e){e.exports={Faq:"FAQs_Faq__KR_c2"}},27984:function(e){e.exports={iframeContainer:"FeaturedApps_iframeContainer__mkokP",Code:"FeaturedApps_Code__8RKDk",Overflow:"FeaturedApps_Overflow__Hx2YA"}},18610:function(e){e.exports={rootEl:"VideoCTA_rootEl__Jksdj"}}},function(e){e.O(0,[29515,22044,98395,22569,1534,21320,59118,49774,92888,40179],function(){return e(e.s=89246)}),_N_E=e.O()}]);